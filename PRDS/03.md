Product Requirements Document (PRD): Docker Containerization for Telequery AI

Version: 1.0

Date: July 15, 2025

Author: Gemini

Overview & Vision
Vision: To enable easy, portable deployment of Telequery AI as a Docker container, allowing users to run the application in isolated environments while accessing external SQLite databases, integrating with external APIs (e.g., OpenAI), and exposing the RESTful API. This will simplify setup for developers and end-users, reduce dependency on local environments, and support production-like deployments.
Problem: The current implementation requires manual setup of Python environments, dependencies, and paths, which can lead to inconsistencies across machines. Deploying on servers or clouds involves repetitive configuration, and handling external data (e.g., databases) is error-prone without standardization.

Solution: Containerize the application using Docker, with support for mounting external databases, passing environment variables for API keys and paths, and a simple shell script for one-command startup. The container will read external SQLite files (main and expansion databases), call external LLM APIs, and expose the FastAPI endpoints. This PRD focuses on the Docker implementation; core app logic remains unchanged.

Scope: Dockerfile creation, volume mounts for databases, environment variable handling, and a startup script. Integration with existing code (e.g., updating path handling in src/api/app.py, src/tools/search.py, etc.) to use configurable paths.

Target Audience
Primary: Developers and Telegram bot integrators who need to deploy Telequery AI quickly on local machines, servers, or cloud platforms (e.g., Docker Compose, Kubernetes).
Secondary: End-users or teams managing Telegram data archives who want a plug-and-play container without deep Python knowledge.

Technology Stack
Containerization: Docker (latest stable version).
Base Image: python:3.10-slim (lightweight, with Python 3.10+).
App Framework: Existing FastAPI/Uvicorn.
Database Handling: SQLite via SQLAlchemy, with paths configurable via env vars.
Vector Search: ChromaDB, with persistent directory mountable as a volume.
LLM Integration: Existing agnostic wrapper (OpenAI/Anthropic).
Scripting: Bash shell script for startup.
Core Features & Functionality
4.1. Dockerfile for Building the Image
The Dockerfile will create a self-contained image with all dependencies installed, copying the source code, and setting Uvicorn as the entrypoint.
Action: Create a new file at project root: Dockerfile.

Content:

dockerfile

Collapse

Wrap

Copy
# Use official Python base image (slim for smaller size)  
FROM python:3.10-slim  

# Set working directory  
WORKDIR /app  

# Install system dependencies (e.g., for SQLite)  
RUN apt-get update && apt-get install -y --no-install-recommends \  
    build-essential \  
    libsqlite3-dev \  
    && rm -rf /var/lib/apt/lists/*  

# Copy requirements and install dependencies  
COPY requirements.txt .  
RUN pip install --no-cache-dir -r requirements.txt  

# Copy application code  
COPY . .  

# Expose the API port  
EXPOSE 8000  

# Run the application (remove --reload for production)  
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
4.2. External Database Mounting

Support two separate database paths: main (telegram_messages.db) and expansion (telequery_expansions.db). These will be mounted as volumes during docker run, allowing external access. Update code to use env vars for paths.

Action: Update .env.example to include:

env

Collapse

Wrap

Copy
MAIN_DB_PATH=/app/data/telegram_messages.db  
EXPANSION_DB_PATH=/app/data/telequery_expansions.db  
VECTOR_INDEX_PATH=/app/data/message_embeddings.index
Action: Modify relevant code files to use these env vars (e.g., in src/api/app.py, src/tools/search.py, run_expansion.py):

Replace hard-coded paths with os.getenv("MAIN_DB_PATH", "./data/telegram_messages.db") and similarly for expansion.
In MessageSearchTool.__init__, use os.getenv("EXPANSION_DB_PATH") for expansion_db_url.
4.3. External API Access

No changes needed; container network allows outbound calls. API keys (e.g., OPENAI_API_KEY) will be passed via --env-file or individual --env flags in docker run.

4.4. API Exposure

The container exposes port 8000, mappable to host ports (e.g., -p 8000:8000).

4.5. Startup Shell Script

A Bash script to simplify container startup, hiding defaults like image name, ports, and volumes. It will:

Build the image if not present.
Run the container with default mounts (e.g., current ./data to /app/data).
Accept optional args for DB paths, API keys, etc.
Action: Create a new file: start_docker.sh.

Content:

sh

Collapse

Wrap

Copy
#!/bin/bash  

# Defaults  
IMAGE_NAME="telequery-ai"  
CONTAINER_NAME="telequery-ai-container"  
HOST_PORT=8000  
CONTAINER_PORT=8000  
MAIN_DB_HOST_PATH="$(pwd)/data/telegram_messages.db"  
EXPANSION_DB_HOST_PATH="$(pwd)/data/telequery_expansions.db"  
VECTOR_HOST_PATH="$(pwd)/data"  
ENV_FILE=".env"  

# Parse arguments (e.g., --main-db /path/to/main.db --expansion-db /path/to/exp.db)  
while [[ $# -gt 0 ]]; do  
    case $1 in  
        --main-db) MAIN_DB_HOST_PATH="$2"; shift 2 ;;  
        --expansion-db) EXPANSION_DB_HOST_PATH="$2"; shift 2 ;;  
        --env-file) ENV_FILE="$2"; shift 2 ;;  
        --host-port) HOST_PORT="$2"; shift 2 ;;  
        *) echo "Unknown option $1"; exit 1 ;;  
    esac  
done  

# Build image if not exists  
if ! docker image inspect $IMAGE_NAME > /dev/null 2>&1; then  
    echo "Building Docker image..."  
    docker build -t $IMAGE_NAME .  
fi  

# Stop and remove existing container if running  
docker stop $CONTAINER_NAME > /dev/null 2>&1  
docker rm $CONTAINER_NAME > /dev/null 2>&1  

# Run container  
echo "Starting container..."  
docker run -d \  
    --name $CONTAINER_NAME \  
    -p $HOST_PORT:$CONTAINER_PORT \  
    -v $MAIN_DB_HOST_PATH:/app/data/telegram_messages.db \  
    -v $EXPANSION_DB_HOST_PATH:/app/data/telequery_expansions.db \  
    -v $VECTOR_HOST_PATH:/app/data \  
    --env-file $ENV_FILE \  
    $IMAGE_NAME  

echo "Telequery AI running at http://localhost:$HOST_PORT"  
echo "Logs: docker logs -f $CONTAINER_NAME"
Make the script executable: chmod +x start_docker.sh.

User Flow
Developer:
a. Builds the image via docker build -t telequery-ai . (or script handles it).
b. Runs ./start_docker.sh --main-db /host/path/main.db --expansion-db /host/path/exp.db.
c. Container starts, mounts DBs, loads env vars.
d. Access API at http://localhost:8000/query.
e. Telegram bot forwards requests to the exposed API.
Success Metrics
Build Time: Image builds in < 2 minutes.
Startup Time: Container ready in < 10 seconds.
API Latency: No increase compared to non-containerized.
Error Rate: Zero failures in DB mounting or API calls during tests.
Usability: Script allows startup in one command with defaults.
Future Enhancements
Docker Compose for multi-container setups (e.g., with Nginx proxy).
CI/CD integration (e.g., GitHub Actions to build/push images).
Health checks in Dockerfile for orchestration tools.
Support for secrets management (e.g., Docker secrets for API keys).
