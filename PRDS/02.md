Of course. Here is the full, revised Product Requirements Document for implementation.

Product Requirements Document (PRD): Context-Aware Message Indexing
Version: 1.4
Date: July 14, 2025
Author: Gemini

Objective: To significantly improve semantic search accuracy by creating and indexing a contextually-aware version of each Telegram message. This document details the necessary changes to be implemented by an AI coding model.

1. Introduction & Problem Statement
The search_relevant_messages tool is fundamentally limited by the nature of conversational messaging. Individual messages often lack full context (e.g., "Yes, I agree." or "What about the cost?"), making them nearly impossible to find via semantic search queries about the original topic. The necessary context exists in preceding messages but is not captured in the message's text itself.

This feature will solve this by creating a new indexing pipeline that rewrites each message to include its surrounding conversational context, making it a self-contained, searchable unit.

2. Architecture: Separate & Persistent Data Store
To ensure our generated data is safe from external processes that might overwrite the source database, all new data will be stored in a separate, dedicated SQLite database.

Source Database (Read-Only): telegram_messages.db. We will treat this as a read-only source of messages.

Expansion Database (Read/Write): telequery_expansions.db. This new database, managed by our application, will store the LLM-generated expanded text and related metadata.

This architecture protects our work and allows each database to serve its primary purpose.

3. Technical Implementation Plan
3.1. New Database for Message Expansions
A new database and schema are required to store the expanded message content.

Action: Create a new environment variable in .env.example to manage the path for this new database.

Code snippet

# .env.example
# ... existing variables
DATABASE_PATH=./data/telegram_messages.db
EXPANSION_DB_PATH=./data/telequery_expansions.db # <-- Add this line
VECTOR_INDEX_PATH=./data/message_embeddings.index
Action: Create a new schema definition file for the expansion database.

File: src/database/expansion_schema.py

Content:

Python

from sqlalchemy import Column, String, DateTime, Text, Index
from sqlalchemy.ext.declarative import declarative_base
import datetime

Base = declarative_base()

class MessageExpansion(Base):
    __tablename__ = 'message_expansions'

    # Use the original message_id as our primary key
    message_id = Column(String, primary_key=True)

    # The LLM-generated expanded text
    expanded_text = Column(Text, nullable=False)

    # Metadata for tracking which model was used
    model_used = Column(String, nullable=False)

    # Timestamp for when the expansion was created
    created_at = Column(DateTime, default=datetime.datetime.utcnow)

    __table_args__ = (
        Index('idx_created_at', 'created_at'),
    )
3.2. New Indexing & Contextualization Module
This module will contain the core logic for generating the expanded text.

Action: Create a new file for the MessageContextualizer.

File: src/indexing/contextualizer.py

Content:

Python

import asyncio
from typing import List
from sqlalchemy.orm import Session
from ..models.database import TelegramMessage
from ..models.schema import Message as SourceMessage
from ..database.expansion_schema import MessageExpansion
from ..llm.factory import LLMFactory, LLMProvider

class MessageContextualizer:
    def __init__(self, source_session: Session, expansion_session: Session):
        self.source_session = source_session
        self.expansion_session = expansion_session
        self.llm_provider: LLMProvider = LLMFactory.create_provider()

    def _get_context_messages(self, message: TelegramMessage, window_size: int = 5) -> List[SourceMessage]:
        """Fetches preceding messages and the replied-to message from the source DB."""
        context_messages = []

        # Fetch the replied-to message if it exists
        if message.reply_to_message_id:
            replied_msg = self.source_session.query(SourceMessage).get(message.reply_to_message_id)
            if replied_msg:
                context_messages.append(replied_msg)

        # Fetch the window of messages before the target message
        recent_msgs = self.source_session.query(SourceMessage).filter(
            SourceMessage.chat_id == message.chat_id,
            SourceMessage.timestamp < message.timestamp
        ).order_by(SourceMessage.timestamp.desc()).limit(window_size).all()

        context_messages.extend(recent_msgs)

        # Remove duplicates and sort chronologically
        final_context = {msg.message_id: msg for msg in context_messages}
        return sorted(final_context.values(), key=lambda m: m.timestamp)

    def _create_expansion_prompt(self, target_message: TelegramMessage, context_messages: List[SourceMessage]) -> str:
        """Creates the specific prompt for the LLM."""
        context_str = "\n".join(
            [f"- {msg.sender_name} at {msg.timestamp.strftime('%H:%M')}: {msg.text}" for msg in context_messages]
        )

        prompt = f"""
        Given the following conversational context from a Telegram chat:
        --- CONTEXT ---
        {context_str}
        --- END CONTEXT ---

        Now, analyze this target message sent by "{target_message.sender_name}":
        --- TARGET MESSAGE ---
        "{target_message.text}"
        --- END TARGET MESSAGE ---

        Your task is to rewrite the TARGET MESSAGE into a single, self-contained sentence or paragraph. This rewritten version must incorporate all necessary information from the CONTEXT so that it can be understood on its own, without seeing the rest of the conversation. Do not add any new information. Focus only on what is being said or asked in the target message itself, using the context to clarify its meaning.
        """
        return prompt

    async def expand_and_save_message(self, message: TelegramMessage):
        """Generates expanded text and saves it to the expansion database."""
        context = self._get_context_messages(message)

        # If there's no text in the message, no need to expand
        if not message.text or not message.text.strip():
            return

        user_prompt = self._create_expansion_prompt(message, context)

        system_prompt = "You are an AI assistant that rewrites a Telegram message to include all necessary context from the preceding conversation, making it a standalone, self-contained statement."

        response = await self.llm_provider.generate_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=0.1
        )

        expansion = MessageExpansion(
            message_id=message.message_id,
            expanded_text=response.content,
            model_used=response.model
        )

        self.expansion_session.add(expansion)
        self.expansion_session.commit()
3.3. Batch Processing Script
A standalone script is required to run the expansion process on messages in the database.

Action: Create a new top-level script to run the expansion process.

File: run_expansion.py

Content:

Python

import asyncio
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from src.models.schema import Message as SourceMessage
from src.database.expansion_schema import MessageExpansion
from src.indexing.contextualizer import MessageContextualizer
from src.models.database import TelegramMessage

load_dotenv()

SOURCE_DB_URL = os.getenv("DATABASE_PATH", "sqlite:///./data/telegram_messages.db")
EXPANSION_DB_URL = os.getenv("EXPANSION_DB_PATH", "sqlite:///./data/telequery_expansions.db")

source_engine = create_engine(f"sqlite:///{SOURCE_DB_URL}")
expansion_engine = create_engine(f"sqlite:///{EXPANSION_DB_URL}")

SourceSession = sessionmaker(bind=source_engine)
ExpansionSession = sessionmaker(bind=expansion_engine)

async def process_batch(limit=1000):
    """
    Finds messages in the source DB that have not yet been expanded
    and processes them.
    """
    source_session = SourceSession()
    expansion_session = ExpansionSession()

    try:
        # Find which message IDs have already been processed
        processed_ids_query = expansion_session.query(MessageExpansion.message_id).all()
        processed_ids = {pid[0] for pid in processed_ids_query}

        print(f"Found {len(processed_ids)} already expanded messages.")

        # Find a batch of unprocessed messages from the source database
        messages_to_process = source_session.query(SourceMessage).filter(
            SourceMessage.message_id.notin_(processed_ids),
            SourceMessage.text != None
        ).order_by(SourceMessage.timestamp.asc()).limit(limit).all()

        if not messages_to_process:
            print("‚úÖ No new messages to expand. Database is up-to-date.")
            return

        print(f"Found {len(messages_to_process)} new messages to expand. Starting process...")

        contextualizer = MessageContextualizer(source_session, expansion_session)

        for i, message in enumerate(messages_to_process):
            # Adapt SQLAlchemy model to Pydantic model for type safety
            pydantic_message = TelegramMessage.from_orm(message)
            try:
                await contextualizer.expand_and_save_message(pydantic_message)
                print(f"({i+1}/{len(messages_to_process)}) Expanded message: {message.message_id}")
            except Exception as e:
                print(f"‚ùóÔ∏è Error expanding message {message.message_id}: {e}")

    finally:
        source_session.close()
        expansion_session.close()

if __name__ == "__main__":
    # Create the expansion table if it doesn't exist
    from src.database.expansion_schema import Base
    Base.metadata.create_all(expansion_engine)
    print("Expansion database and table verified.")

    asyncio.run(process_batch())
3.4. Update Search Indexing Logic
The MessageSearchTool must be updated to use the new expanded_text.

Action: Modify the search tool to connect to the expansion DB and retrieve expanded text.

File: src/tools/search.py

Change:

Python

# ... other imports
from ..database.expansion_schema import MessageExpansion

class MessageSearchTool:
    def __init__(self, chroma_path: str = "./chroma_db", database_url: str = "sqlite:///./telegram_messages.db", expansion_db_url: str = "sqlite:///./telequery_expansions.db"):
        # ... existing __init__ code ...

        # --- NEW ---
        # Add a connection to the expansion database
        self.expansion_engine = create_engine(expansion_db_url)
        self.ExpansionSessionLocal = sessionmaker(bind=self.expansion_engine)
        # --- END NEW ---

    # ... existing methods ...

    def _get_searchable_text(self, message: TelegramMessage, expansion_session: Session) -> str:
        """
        Fetches the expanded text if it exists; otherwise, returns the original text.
        This is a helper to be used within populate_search_index.
        """
        expansion = expansion_session.query(MessageExpansion).get(message.message_id)
        if expansion and expansion.expanded_text:
            return expansion.expanded_text
        return message.text

    def populate_search_index(self):
        """Populate the search index with all messages from the database."""
        print("üîç Populating search index from database...")

        source_session = self.SessionLocal()
        expansion_session = self.ExpansionSessionLocal()

        try:
            # Logic to fetch all messages from the source database
            all_source_messages = source_session.query(SourceMessage).all()

            # ... logic to clear existing Chroma collection ...

            documents = []
            metadatas = []
            ids = []

            for message in all_source_messages:
                pydantic_message = TelegramMessage.from_orm(message)

                # Use the helper to get the best text for searching
                searchable_text = self._get_searchable_text(pydantic_message, expansion_session)

                # Skip if there's no text to index
                if not searchable_text:
                    continue

                documents.append(searchable_text)
                metadatas.append({
                    "message_id": pydantic_message.message_id,
                    "chat_id": pydantic_message.chat_id,
                    "sender_name": pydantic_message.sender_name,
                    "timestamp": pydantic_message.timestamp.isoformat()
                })
                ids.append(pydantic_message.message_id)

            if documents:
                # Batch add to ChromaDB
                self.collection.add(
                    ids=ids,
                    documents=documents,
                    metadatas=metadatas
                )
                print(f"‚úÖ Added/updated {len(documents)} messages in the search index.")

        finally:
            source_session.close()
            expansion_session.close()

# Update the get_search_tool factory function to pass the new DB path
def get_search_tool(database_url: str, chroma_path: str, expansion_db_url: str):
    # ... update to pass expansion_db_url to the MessageSearchTool constructor ...
